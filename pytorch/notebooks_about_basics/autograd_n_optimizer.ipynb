{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensor vs Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import Tensor \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLinear(nn.Module):\n",
    "    def __init__(self, in_features :int, out_features: int, bias=True):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "\n",
    "        self.weights = nn.Parameter(torch.randn(in_features, out_features))\n",
    "        self.bias = nn.Parameter(torch.randn(out_features))\n",
    "\n",
    "    def forward(self, x : Tensor):\n",
    "        return x @ self.weights + self.bias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.7540, -1.4113, -2.7719, -1.2635, -0.1125, -1.6292,  0.6910],\n",
       "        [-0.3963, -0.9371,  0.0390,  0.3860, -1.2742, -1.1699, -0.6987],\n",
       "        [-0.6261,  0.8114, -0.9349,  0.7862,  0.2609, -0.5031, -2.0653],\n",
       "        [-1.3957, -0.7094,  0.1954,  0.6103, -0.9790, -1.0491,  1.6414],\n",
       "        [-0.8234, -1.0884, -0.9798, -0.1425,  0.3820, -0.7066, -1.2968]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(5,7)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 12])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer = MyLinear(7, 12)\n",
    "layer(x)\n",
    "layer(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.4373,  0.9162, -0.9937, -1.4457,  1.2842,  0.3717, -0.5025,  1.8465,\n",
      "         -0.4021, -2.4787, -3.4916,  1.1409],\n",
      "        [-0.1255,  1.3740,  0.5123, -0.5508,  0.3859,  0.3136, -0.6276,  1.0184,\n",
      "         -1.7447,  0.7766,  0.3809,  1.1429],\n",
      "        [ 1.2965, -0.2828, -0.7173, -0.1925,  0.0815, -0.2593,  1.0278,  0.1479,\n",
      "         -0.9371,  0.6382, -0.1692,  0.3042],\n",
      "        [-0.1922, -1.7373, -0.4830,  0.0230, -2.6770, -1.1562, -0.4834, -0.5233,\n",
      "          0.7815, -0.7825,  0.1765, -2.9611],\n",
      "        [-0.0835,  0.6990,  0.2747, -0.5984, -0.8869, -0.4160, -1.8068, -1.2907,\n",
      "         -0.8190, -0.4657, -0.3774, -0.8374],\n",
      "        [-0.1414, -0.5227, -0.1598,  1.3709,  0.2292,  2.7624,  1.4704, -0.6335,\n",
      "          1.1171,  0.5894, -0.5471,  0.3719],\n",
      "        [-0.7047,  0.4909,  0.7605, -1.3339, -0.0253, -0.9228,  0.0213,  0.0497,\n",
      "         -0.1394,  0.9112, -0.5770, -0.6991]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.4597, -0.1415, -1.4350,  1.1516, -1.0203, -2.3060, -0.1727,  0.4134,\n",
      "         0.9448, -0.8579,  0.4199,  0.5436], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for value in layer.parameters():\n",
    "    print(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.9042, -6.1153,  3.8754,  2.7142, -4.9368,  4.9888,  2.3972,  7.7085,\n",
       "          4.5700, -1.0603, -2.0727, -0.8741],\n",
       "        [-1.6594,  0.4161,  0.8331,  3.0138, -2.2929,  3.7931,  2.5698,  3.5831,\n",
       "          2.8548,  3.0971, -1.5520,  1.7841],\n",
       "        [ 0.5013, -0.6970,  1.1580,  3.8808,  2.0870,  0.8639, -5.7337, -1.8347,\n",
       "          5.5430,  3.4776, -3.3267,  3.1814],\n",
       "        [-1.1581,  1.9436, -2.5082, -0.3302, -4.6993,  4.7770,  4.6783,  2.7982,\n",
       "          1.0504,  0.5612, -1.1633, -2.5134],\n",
       "        [-0.4042, -1.6464,  3.4230,  3.7326, -2.3025,  1.4835, -2.4468,  3.7676,\n",
       "          2.8090,  1.6334, -2.7531,  0.1628]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MyLinearTensor(nn.Module):\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features \n",
    "\n",
    "        self.weights = Tensor(torch.randn(in_features, out_features))\n",
    "        self.bias = Tensor(torch.randn(out_features))\n",
    "\n",
    "    def forward(self, x : Tensor):\n",
    "        return x @ self.weights + self.bias\n",
    "\n",
    "layerT = MyLinear(7, 12)\n",
    "layerT(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 12])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layerT(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 1.0044e+00, -4.5313e-01, -3.3337e-01,  5.3342e-01,  6.0799e-01,\n",
      "         -1.5472e+00,  2.9942e-01,  8.3805e-01, -3.7134e-01,  6.5981e-02,\n",
      "          1.6511e+00,  1.2281e+00],\n",
      "        [-2.0094e-01,  4.2677e-01, -5.4562e-01, -9.3851e-03,  1.8258e+00,\n",
      "          5.3629e-02, -2.1525e-01, -2.7510e+00,  7.7101e-01,  1.6096e-01,\n",
      "         -3.2056e-02,  1.1876e+00],\n",
      "        [ 2.6770e-01,  2.2710e+00, -2.0812e-01,  3.9148e-03, -6.4484e-02,\n",
      "         -6.8611e-01,  1.4791e+00, -6.2586e-01, -1.8502e+00, -3.5231e-02,\n",
      "          5.4698e-01,  1.3511e-01],\n",
      "        [ 1.6111e+00,  2.6095e-01, -2.3182e+00, -7.8799e-01, -6.9408e-02,\n",
      "         -4.3716e-01, -1.8725e+00,  1.3119e-01,  9.9571e-01,  4.1554e-01,\n",
      "         -3.3456e-01, -1.9611e-01],\n",
      "        [ 1.7729e+00,  1.4838e-01,  7.3569e-02,  3.7934e-03, -2.4655e-01,\n",
      "         -2.1238e+00, -1.6422e+00,  3.0338e-01, -8.5535e-01, -1.3915e+00,\n",
      "          9.1688e-02, -5.2302e-01],\n",
      "        [ 2.3124e-03,  2.9802e-01,  6.9491e-02, -2.9685e-01,  5.8972e-01,\n",
      "         -6.2532e-01, -1.7892e+00, -1.5714e+00, -1.1117e+00,  1.0503e+00,\n",
      "          4.8753e-01, -1.6702e+00],\n",
      "        [ 2.6661e-01,  2.0680e-01, -1.2939e+00, -1.1102e+00, -9.3477e-01,\n",
      "          1.4249e-01,  1.4301e+00,  3.6232e-01, -8.1114e-01, -9.8735e-01,\n",
      "          8.3308e-01, -1.2656e+00]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.3661,  1.1293,  0.3636,  2.4023, -0.5891,  0.0876, -0.0347,  0.1125,\n",
      "         0.1608,  1.8809,  0.4495, -0.0506], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for value in layerT.parameters():\n",
    "    print(value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AutoGrad for Linear Regression\n",
    "\n",
    "$$\n",
    "    y = 2x + 1\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.],\n",
       "       [ 1.],\n",
       "       [ 2.],\n",
       "       [ 3.],\n",
       "       [ 4.],\n",
       "       [ 5.],\n",
       "       [ 6.],\n",
       "       [ 7.],\n",
       "       [ 8.],\n",
       "       [ 9.],\n",
       "       [10.]], dtype=float32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# create dummy data for training \n",
    "x_values = [i for i in range(11)]\n",
    "x_train = np.array(x_values, dtype=np.float32)\n",
    "x_train = x_train.reshape(-1,1) # ROW로 만들어줌 (Col이 1인 ROW 여러 개 의미)\n",
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_values = [ 2*x + 1 for x in x_values]\n",
    "y_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.],\n",
       "       [ 3.],\n",
       "       [ 5.],\n",
       "       [ 7.],\n",
       "       [ 9.],\n",
       "       [11.],\n",
       "       [13.],\n",
       "       [15.],\n",
       "       [17.],\n",
       "       [19.],\n",
       "       [21.]], dtype=float32)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train = np.array(y_values, dtype=np.float32)\n",
    "y_train = y_train.reshape(-1, 1)\n",
    "y_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "HIDDEN_DIM = 1\n",
    "\n",
    "class LinearRegression(torch.nn.Module):\n",
    "    def __init__(self, inputSize, outputSize):\n",
    "        super(LinearRegression, self).__init__()\n",
    "        self.linear = torch.nn.Linear(inputSize, outputSize)\n",
    "        #self.linear2 = torch.nn.Linear(HIDDEN_DIM, outputSize)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.linear(x)\n",
    "        #out = self.linear2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputDim = 1 # take variable 'x' \n",
    "outputDim = 1 # takes variable 'y'  \n",
    "learningRate = 0.01 \n",
    "epochs = 100\n",
    "\n",
    "model = LinearRegression(inputDim, outputDim)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learningRate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, loss 62.466346740722656\n",
      "epoch 1, loss 5.0957722663879395\n",
      "epoch 2, loss 0.4162299931049347\n",
      "epoch 3, loss 0.03452688083052635\n",
      "epoch 4, loss 0.003386527067050338\n",
      "epoch 5, loss 0.0008400771766901016\n",
      "epoch 6, loss 0.0006260964437387884\n",
      "epoch 7, loss 0.0006024108733981848\n",
      "epoch 8, loss 0.0005943237338215113\n",
      "epoch 9, loss 0.0005875776405446231\n",
      "epoch 10, loss 0.0005810072761960328\n",
      "epoch 11, loss 0.0005745263770222664\n",
      "epoch 12, loss 0.0005680934991687536\n",
      "epoch 13, loss 0.0005617544520646334\n",
      "epoch 14, loss 0.0005554808885790408\n",
      "epoch 15, loss 0.0005492804921232164\n",
      "epoch 16, loss 0.0005431605386547744\n",
      "epoch 17, loss 0.0005370787112042308\n",
      "epoch 18, loss 0.0005310876877047122\n",
      "epoch 19, loss 0.0005251556285656989\n",
      "epoch 20, loss 0.000519289867952466\n",
      "epoch 21, loss 0.0005134879029355943\n",
      "epoch 22, loss 0.0005077639943920076\n",
      "epoch 23, loss 0.0005020847893320024\n",
      "epoch 24, loss 0.0004964863765053451\n",
      "epoch 25, loss 0.0004909386043436825\n",
      "epoch 26, loss 0.0004854646685998887\n",
      "epoch 27, loss 0.0004800407914444804\n",
      "epoch 28, loss 0.00047467334661632776\n",
      "epoch 29, loss 0.000469380640424788\n",
      "epoch 30, loss 0.00046412908704951406\n",
      "epoch 31, loss 0.0004589543677866459\n",
      "epoch 32, loss 0.0004538213543128222\n",
      "epoch 33, loss 0.0004487497790250927\n",
      "epoch 34, loss 0.00044374336721375585\n",
      "epoch 35, loss 0.00043878264841623604\n",
      "epoch 36, loss 0.00043388278572820127\n",
      "epoch 37, loss 0.0004290436627343297\n",
      "epoch 38, loss 0.00042425503488630056\n",
      "epoch 39, loss 0.00041952208266593516\n",
      "epoch 40, loss 0.0004148277221247554\n",
      "epoch 41, loss 0.0004101974773220718\n",
      "epoch 42, loss 0.00040561193600296974\n",
      "epoch 43, loss 0.00040108730900101364\n",
      "epoch 44, loss 0.00039661332266405225\n",
      "epoch 45, loss 0.0003921787429135293\n",
      "epoch 46, loss 0.0003878005954902619\n",
      "epoch 47, loss 0.0003834723320323974\n",
      "epoch 48, loss 0.00037919299211353064\n",
      "epoch 49, loss 0.00037496016011573374\n",
      "epoch 50, loss 0.00037076498847454786\n",
      "epoch 51, loss 0.0003666312259156257\n",
      "epoch 52, loss 0.0003625308454502374\n",
      "epoch 53, loss 0.0003584753430914134\n",
      "epoch 54, loss 0.0003544736246112734\n",
      "epoch 55, loss 0.0003505280474200845\n",
      "epoch 56, loss 0.00034661611425690353\n",
      "epoch 57, loss 0.00034274064819328487\n",
      "epoch 58, loss 0.00033890194026753306\n",
      "epoch 59, loss 0.00033513110247440636\n",
      "epoch 60, loss 0.0003313811030238867\n",
      "epoch 61, loss 0.00032768622622825205\n",
      "epoch 62, loss 0.00032401870703324676\n",
      "epoch 63, loss 0.00032039900543168187\n",
      "epoch 64, loss 0.0003168314869981259\n",
      "epoch 65, loss 0.00031328678596764803\n",
      "epoch 66, loss 0.0003097972075920552\n",
      "epoch 67, loss 0.00030633111600764096\n",
      "epoch 68, loss 0.00030290495487861335\n",
      "epoch 69, loss 0.00029952896875329316\n",
      "epoch 70, loss 0.0002961870632134378\n",
      "epoch 71, loss 0.00029287036159075797\n",
      "epoch 72, loss 0.0002896028745453805\n",
      "epoch 73, loss 0.00028637220384553075\n",
      "epoch 74, loss 0.0002831819874700159\n",
      "epoch 75, loss 0.0002800136571750045\n",
      "epoch 76, loss 0.00027688464615494013\n",
      "epoch 77, loss 0.00027378660161048174\n",
      "epoch 78, loss 0.00027073282399214804\n",
      "epoch 79, loss 0.0002677202573977411\n",
      "epoch 80, loss 0.00026471895398572087\n",
      "epoch 81, loss 0.0002617664576973766\n",
      "epoch 82, loss 0.0002588497882243246\n",
      "epoch 83, loss 0.00025595439365133643\n",
      "epoch 84, loss 0.00025309843476861715\n",
      "epoch 85, loss 0.00025027041556313634\n",
      "epoch 86, loss 0.0002474789216648787\n",
      "epoch 87, loss 0.0002447135921102017\n",
      "epoch 88, loss 0.00024197172024287283\n",
      "epoch 89, loss 0.00023928041628096253\n",
      "epoch 90, loss 0.00023660552687942982\n",
      "epoch 91, loss 0.0002339634665986523\n",
      "epoch 92, loss 0.00023135110677685589\n",
      "epoch 93, loss 0.00022877058654557914\n",
      "epoch 94, loss 0.000226213873247616\n",
      "epoch 95, loss 0.00022368179634213448\n",
      "epoch 96, loss 0.00022119317145552486\n",
      "epoch 97, loss 0.0002187196514569223\n",
      "epoch 98, loss 0.00021626953093800694\n",
      "epoch 99, loss 0.0002138622512575239\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    if torch.cuda.is_available():\n",
    "        inputs = Variable(torch.from_numpy(x_train).cuda())\n",
    "        labels = Variable(torch.from_numpy(y_train).cuda())\n",
    "    else:\n",
    "        inputs = Variable(torch.from_numpy(x_train))\n",
    "        labels = Variable(torch.from_numpy(y_train))\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    outputs = model(inputs)\n",
    "\n",
    "    loss = criterion(outputs, labels)\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "\n",
    "    print(f'epoch {epoch}, loss {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9727964] tensor([1.])\n",
      "[2.9767141] tensor([3.])\n",
      "[4.980632] tensor([5.])\n",
      "[6.9845495] tensor([7.])\n",
      "[8.988467] tensor([9.])\n",
      "[10.992385] tensor([11.])\n",
      "[12.996303] tensor([13.])\n",
      "[15.00022] tensor([15.])\n",
      "[17.004137] tensor([17.])\n",
      "[19.008055] tensor([19.])\n",
      "[21.011972] tensor([21.])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    if torch.cuda.is_available():\n",
    "        predicted = model(Variable(torch.from_numpy(x_train).cuda())).cpu().data.numpy()\n",
    "    else:\n",
    "        predicted = model(Variable(torch.from_numpy(x_train))).data.numpy()\n",
    "\n",
    "\n",
    "for p, l in zip(predicted, labels):\n",
    "    print(p, l)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('torch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "04c55434ed2b08bb5bf53ae5f55862aa0e3be3e7e24f24482bddca5922bab6dd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
